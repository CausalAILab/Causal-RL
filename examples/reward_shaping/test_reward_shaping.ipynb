{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9036d927",
   "metadata": {},
   "source": [
    "# Automatic Reward Shaping from Confounded Offline Data\n",
    "This notebook is based on our ICML 25 [paper](https://openreview.net/forum?id=Hu7hUjEMiW&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICML.cc%2F2025%2FConference%2FAuthors%23your-submissions)).\n",
    "Also see the Techical Report version [here](https://causalai.net/r123.pdf).\n",
    "\n",
    "The task is to learn a potential function automatically from offline data to be used in Potential Based Reward Shaping (PBRS). The new reward function after reward shaping is defined to be,\n",
    "$$\n",
    "Y' = Y + \\gamma\\phi(s') - \\phi(s)\n",
    "$$\n",
    "where $Y$ is the original reward signal and $Y'$ is the one after shaping. $\\phi(\\cdot)$ is the potential function we aim to learn automatically from offline datasets.\n",
    "\n",
    "Intuitively, one can use the optimal state values as the potential function. And if the provided offline dataset is generated by a good enough policy, one can directly take the average cumulative return as the state value estimations. However, when the offline dataset is confounded or the data generating policy is sub-optimal, such naive estimations are highly biased and could mislead the policy training. See example 1&2 in the paper for more details.\n",
    "\n",
    "In this work, we use causal bounds to estimate an upper bound on the optimal interventional\n",
    "state values. Then, we take the estimated upper value bound as the potential function to train our online policy learner, Q-UCB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aea67ce",
   "metadata": {},
   "source": [
    "## Environment Definition and Data Generation\n",
    "In this notebook, we will replicate our experiment results in environment WindyLavaCross (hard), corresponding to Fig. 3(c) and Fig. 4(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a42251",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StrEnum' from 'enum' (/Users/ml/anaconda3/envs/torchdev/lib/python3.9/enum.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausal_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausal_rl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreward_shaping\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalculate_values\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define behavioral policy\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/py/causal/causalgym/causal_gym/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Development/py/causal/causalgym/causal_gym/core/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     SCM,\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     PCH,\n\u001b[1;32m      6\u001b[0m     PCHWrapper,\n\u001b[1;32m      7\u001b[0m     ActionPCHWrapper,\n\u001b[1;32m      8\u001b[0m     ObservationPCHWrapper,\n\u001b[1;32m      9\u001b[0m     RewardPCHWrapper,\n\u001b[1;32m     10\u001b[0m     PolicyPCHWrapper,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     PolicyScope\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     Task\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/Development/py/causal/causalgym/causal_gym/core/pch.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SCM\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Assumptions, LearningRegime, Task\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPCH\u001b[39;00m(    \n\u001b[1;32m     18\u001b[0m     Wrapper[WrapperObsType, WrapperActType, ObsType, ActType],\n\u001b[1;32m     19\u001b[0m     Generic[WrapperPolicyType, WrapperObsType, WrapperActType, PolicyType, ObsType, ActType],\n\u001b[1;32m     20\u001b[0m ):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    The main class for interacting with SCMs.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    Currently we support L1 and L2 interactions, namely,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        - :meth:`see` - Updates an environment following the behavior policy returning the realized action, the next agent observation, the reward for taking that actions,\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        - :meth:`do` - Updates an environment with actions returning the next agent observation, the reward for taking that actions.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/py/causal/causalgym/causal_gym/core/task.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntEnum, StrEnum\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Generic, SupportsFloat, TypeVar, Union\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StrEnum' from 'enum' (/Users/ml/anaconda3/envs/torchdev/lib/python3.9/enum.py)"
     ]
    }
   ],
   "source": [
    "from causal_gym.core import Task\n",
    "from causal_rl.algo.reward_shaping.calculate_values import *\n",
    "\n",
    "# Define behavioral policy\n",
    "BEHAVIORAL = {\n",
    "    'Nowind-Empty-8x8-v0': {\n",
    "        'good': good_bpolicy_emptyworld,\n",
    "        'bad': lambda s, w: good_bpolicy_emptyworld(s, w) if np.random.rand() > .5 else np.random.choice(5),\n",
    "        'random': lambda s, w: np.random.choice(5),\n",
    "    },\n",
    "    'MiniGrid-Empty-8x8-v0': {\n",
    "        'good': good_bpolicy_emptyworld,\n",
    "        'bad': lambda s, w: good_bpolicy_emptyworld(s, w) if np.random.rand() > .5 else np.random.choice(5),\n",
    "        'random': lambda s, w: np.random.choice(5),\n",
    "    },\n",
    "    'Custom-LavaCrossing-easy-v0': {\n",
    "        'good': good_bpolicy_lavacross,\n",
    "        'bad': bad_bpolicy_lavacross,\n",
    "        'random': lambda s, w: np.random.choice(5)\n",
    "    },\n",
    "    'Custom-LavaCrossing-hard-v0': {\n",
    "        'good': good_bpolicy_lavacross_hard,\n",
    "        'bad': bad_bpolicy_lavacross_hard,\n",
    "        'random': lambda s, w: np.random.choice(5)\n",
    "    },\n",
    "    'Custom-LavaCrossing-extreme-v0': {\n",
    "        'good': good_bpolicy_lavacross_extreme,\n",
    "        'bad': bad_bpolicy_lavacross_extreme,\n",
    "        'random': lambda s, w: np.random.choice(5)\n",
    "    },\n",
    "    'Custom-LavaCrossing-maze-v0': {\n",
    "        'good': good_bpolicy_lavacross_maze,\n",
    "        'bad': bad_bpolicy_lavacross_maze,\n",
    "        'bad2': bad_bpolicy_lavacross_maze2\n",
    "    },\n",
    "    'Custom-LavaCrossing-maze-complex-v0': {\n",
    "        'good': better_bpolicy_lavacross_maze_complex,\n",
    "        'bad': good_bpolicy_lavacross_maze,\n",
    "        'bad2': bad_bpolicy_lavacross_maze_complex\n",
    "    }\n",
    "}\n",
    "\n",
    "env_name = 'Custom-LavaCrossing-extreme-v0'\n",
    "for SEED in SEEDS:\n",
    "    print('\\n=======================================\\n')\n",
    "    print(f'Env: {env_name} Seed: {SEED}')\n",
    "    # Initialize the environment\n",
    "    env = gym.make(\n",
    "        env_name, \n",
    "        agent_pov=False, \n",
    "        render_mode='rgb_array', \n",
    "        highlight=False, \n",
    "        **KWARGS[env_name]\n",
    "    )\n",
    "    # We have to use 'cool' as the learning regime since we need do for online learning\n",
    "    # and see for collecting offline data.\n",
    "    windy_env = MiniGridActionRemapWrapper(WindyMiniGridPCH(\n",
    "        env=env, \n",
    "        show_wind=True, \n",
    "        wind_dist=WIND_DIST[env_name],\n",
    "        task=Task(learning_regime='cool'), \n",
    "    ))\n",
    "    # Calculate optimal interventional policy space state value\n",
    "    opt_values, opt_qvalues = value_iteration(windy_env)\n",
    "    print(f'Opt state values of {env_name}')\n",
    "    print(np.transpose(opt_values))\n",
    "    save_values(opt_values, f'OPTV-{env_name}-{SEED}')\n",
    "    save_values(opt_qvalues, f'OPTQ-{env_name}-{SEED}')\n",
    "    print('------------------------')\n",
    "\n",
    "    bounds = []\n",
    "    mixed_dataset = []\n",
    "    for policy_name, bpolicy in BEHAVIORAL[env_name].items():\n",
    "        dataset, behavioral_values = gen_dataset(windy_env, bpolicy, seed=SEED)\n",
    "        mixed_dataset.extend(dataset)\n",
    "        print(f'{policy_name} behavioral policy values')\n",
    "        print(np.transpose(behavioral_values))\n",
    "        save_values(behavioral_values, f'BEV-{policy_name}-{env_name}-{SEED}')\n",
    "        print('------------------------')\n",
    "        bound, state_count = approx_opt_value_upper_bound(windy_env, dataset, windy_env.state_space, windy_env.action_space.n, horizon=KWARGS[env_name]['max_episode_steps'], reward_upper_bound=0)\n",
    "        print(f'{policy_name} behavioral policy value bounds in {env_name}')\n",
    "        print(np.transpose(bound))\n",
    "        print('------------------------')\n",
    "        bounds.append(bound)\n",
    "        save_values(bound, f'BD-{policy_name}-{env_name}-{SEED}')\n",
    "\n",
    "    with open(f'data/mixdata-{env_name}-{SEED}.json', 'w') as f:\n",
    "        json.dump(mixed_dataset, f)\n",
    "    final_bound = np.minimum.reduce(bounds)\n",
    "    print(f'\\nFinal Bound for {env_name}:')\n",
    "    print(np.transpose(final_bound))\n",
    "    save_values(final_bound, f'BD-FINAL-{env_name}-{SEED}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
