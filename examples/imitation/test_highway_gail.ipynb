{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc39db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from causal_gym import HighwayPCH\n",
    "from causal_rl.algo.imitation.imitate import parse_graph, find_sequential_pi_backdoor, collect_expert_trajectories\n",
    "from causal_rl.algo.imitation.gail.core_net import DiscreteActor, Critic, Discriminator\n",
    "from causal_rl.algo.imitation.gail.causal_gail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 25\n",
    "seed = 1\n",
    "train_eps = 1000\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2344c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HighwayPCH(num_steps=num_steps, seed=seed, render_mode='rgb_array')\n",
    "num_actions = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ecf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = parse_graph(env.get_graph)\n",
    "X = {f'X{t}' for t in range(num_steps)}\n",
    "Y = f'Y{num_steps}'\n",
    "obs_prefix = env.env.observed_unobserved_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6de747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_dim = 370  | encode(dummy_obs, 0).size = 370\n"
     ]
    }
   ],
   "source": [
    "Z_sets = find_sequential_pi_backdoor(G, X, Y, obs_prefix)\n",
    "categorical_dims = calc_categorical_dims(env)\n",
    "dummy_obs, _ = env.reset(seed=seed)\n",
    "\n",
    "encode, z_dim, union_tokens, var_dims = build_z_encoder(Z_sets, dummy_obs, categorical_dims)\n",
    "dummy_z = encode(dummy_obs, 0)\n",
    "print('z_dim =', z_dim, ' | encode(dummy_obs, 0).size =', int(np.asarray(dummy_z).size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68234c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1/1000...\n",
      "  Episode 1 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 2/1000...\n",
      "  Episode 2 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 3/1000...\n",
      "  Episode 3 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 4/1000...\n",
      "  Episode 4 ended at step 10 (terminated: True, truncated: False).\n",
      "Starting episode 5/1000...\n",
      "  Episode 5 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 6/1000...\n",
      "  Episode 6 ended at step 17 (terminated: True, truncated: False).\n",
      "Starting episode 7/1000...\n",
      "  Episode 7 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 8/1000...\n",
      "  Episode 8 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 9/1000...\n",
      "  Episode 9 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 10/1000...\n",
      "  Episode 10 ended at step 7 (terminated: True, truncated: False).\n",
      "Starting episode 11/1000...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m records = \u001b[43mcollect_expert_trajectories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbehavioral_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\Desktop\\COMS4995 Causal Inference II\\causalrl\\imitation\\imitate.py:234\u001b[39m, in \u001b[36mcollect_expert_trajectories\u001b[39m\u001b[34m(env, num_episodes, max_steps, behavioral_policy, seed, reset_seed_fn)\u001b[39m\n\u001b[32m    231\u001b[39m env.reset(seed=ep_seed)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     obs, reward, terminated, truncated, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msee\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbehavioral_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbehavioral_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_reward\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     action = info[\u001b[33m'\u001b[39m\u001b[33mnatural_action\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    241\u001b[39m     trajs.append({\n\u001b[32m    242\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepisode\u001b[39m\u001b[33m'\u001b[39m: ep,\n\u001b[32m    243\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m'\u001b[39m: step,\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m         \u001b[33m'\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m'\u001b[39m: copy.deepcopy(info)\n\u001b[32m    250\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\COMS4995 Causal Inference II\\causalgym\\causal_gym\\core\\pch.py:68\u001b[39m, in \u001b[36mPCH.__getattribute__.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m._permission_check(name)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\COMS4995 Causal Inference II\\causalgym\\causal_gym\\envs\\highway.py:586\u001b[39m, in \u001b[36mHighwayPCH.see\u001b[39m\u001b[34m(self, behavioral_policy, show_reward)\u001b[39m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    584\u001b[39m     action = \u001b[38;5;28mself\u001b[39m.env.action(X, D, L, _I, A, B, W)\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_reward\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_reward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    587\u001b[39m info[\u001b[33m'\u001b[39m\u001b[33mnatural_action\u001b[39m\u001b[33m'\u001b[39m] = action\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\COMS4995 Causal Inference II\\causalgym\\causal_gym\\envs\\highway.py:351\u001b[39m, in \u001b[36mHighwaySCM.step\u001b[39m\u001b[34m(self, action, show_reward)\u001b[39m\n\u001b[32m    348\u001b[39m A_t = \u001b[38;5;28mself\u001b[39m.A[\u001b[38;5;28mself\u001b[39m.t]\n\u001b[32m    349\u001b[39m B_t = \u001b[38;5;28mself\u001b[39m.B[\u001b[38;5;28mself\u001b[39m.t]\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m env_obs, _, terminated, _, env_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m Y_t = \u001b[38;5;28mself\u001b[39m._reward(X_t, D_t, A_t, B_t, U_t)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._Y) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:240\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    236\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe road and vehicle must be initialized in the environment implementation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    237\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.time += \u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mpolicy_frequency\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m obs = \u001b[38;5;28mself\u001b[39m.observation_type.observe()\n\u001b[32m    243\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._reward(action)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\envs\\common\\abstract.py:271\u001b[39m, in \u001b[36mAbstractEnv._simulate\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    260\u001b[39m     action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    261\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33mmanual_control\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m     == \u001b[32m0\u001b[39m\n\u001b[32m    268\u001b[39m ):\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_type.act(action)\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[38;5;28mself\u001b[39m.road.step(\u001b[32m1\u001b[39m / \u001b[38;5;28mself\u001b[39m.config[\u001b[33m\"\u001b[39m\u001b[33msimulation_frequency\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    273\u001b[39m \u001b[38;5;28mself\u001b[39m.steps += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:464\u001b[39m, in \u001b[36mRoad.act\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decide the actions of each entity on the road.\"\"\"\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vehicles:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[43mvehicle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\vehicle\\behavior.py:115\u001b[39m, in \u001b[36mIDMVehicle.act\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    110\u001b[39m action[\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m] = np.clip(\n\u001b[32m    111\u001b[39m     action[\u001b[33m\"\u001b[39m\u001b[33msteering\u001b[39m\u001b[33m\"\u001b[39m], -\u001b[38;5;28mself\u001b[39m.MAX_STEERING_ANGLE, \u001b[38;5;28mself\u001b[39m.MAX_STEERING_ANGLE\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# Longitudinal: IDM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m front_vehicle, rear_vehicle = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mneighbour_vehicles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlane_index\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m action[\u001b[33m\"\u001b[39m\u001b[33macceleration\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.acceleration(\n\u001b[32m    119\u001b[39m     ego_vehicle=\u001b[38;5;28mself\u001b[39m, front_vehicle=front_vehicle, rear_vehicle=rear_vehicle\n\u001b[32m    120\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# When changing lane, check both current and target lanes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\road\\road.py:504\u001b[39m, in \u001b[36mRoad.neighbour_vehicles\u001b[39m\u001b[34m(self, vehicle, lane_index)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.vehicles + \u001b[38;5;28mself\u001b[39m.objects:\n\u001b[32m    500\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vehicle \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    501\u001b[39m         v, Landmark\n\u001b[32m    502\u001b[39m     ):  \u001b[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001b[39;00m\n\u001b[32m    503\u001b[39m         \u001b[38;5;66;03m# lane_index, same_lane=True):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m         s_v, lat_v = \u001b[43mlane\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlocal_coordinates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lane.on_lane(v.position, s_v, lat_v, margin=\u001b[32m1\u001b[39m):\n\u001b[32m    506\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eylam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\highway_env\\road\\lane.py:212\u001b[39m, in \u001b[36mStraightLane.local_coordinates\u001b[39m\u001b[34m(self, position)\u001b[39m\n\u001b[32m    210\u001b[39m delta = position - \u001b[38;5;28mself\u001b[39m.start\n\u001b[32m    211\u001b[39m longitudinal = np.dot(delta, \u001b[38;5;28mself\u001b[39m.direction)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m lateral = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdirection_lateral\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(longitudinal), \u001b[38;5;28mfloat\u001b[39m(lateral)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "records = collect_expert_trajectories(\n",
    "    env,\n",
    "    num_episodes=train_eps,\n",
    "    max_steps=num_steps,\n",
    "    behavioral_policy=None,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a47ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('expert_traj_gail.pkl', 'wb') as f:\n",
    "    pickle.dump(records, f)\n",
    "\n",
    "print(f'saved {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ca3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('expert_traj_gail.pkl', 'rb') as f:\n",
    "    records = pickle.load(f)\n",
    "\n",
    "print(f'loaded {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = DiscreteActor(z_dim, num_actions, hidden_size=128).to(device)\n",
    "critic = Critic(z_dim, hidden_size=128).to(device)\n",
    "discriminator = Discriminator(z_dim + num_actions, hidden_size=128, dropout=0.2).to(device)\n",
    "\n",
    "actor_optim = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optim = Adam(critic.parameters(), lr=1e-4)\n",
    "discriminator_optim = Adam(discriminator.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5142, 445]) torch.Size([5142]) torch.Size([5142, 450])\n"
     ]
    }
   ],
   "source": [
    "Z_e, A_e, X_e = make_expert_batch(records, encode, num_actions)\n",
    "print(Z_e.shape, A_e.shape, X_e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9790e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_env_return': 226.13310888900185,\n",
       " 'avg_D_reward': 0.6900997161865234,\n",
       " 'ppo_actor_loss': -0.010333863086998463,\n",
       " 'ppo_critic_loss': 2.1499789357185364,\n",
       " 'ppo_entropy': 1.6092643439769745,\n",
       " 'ppo_approx_kl': -0.00042894088647489614,\n",
       " 'ppo_clip_frac': 0.0,\n",
       " 'D_loss': 10.738677501678467,\n",
       " 'D_real_mean': -0.0033468197216279805,\n",
       " 'D_fake_mean': -0.006157793221063912,\n",
       " 'D_gp': 0.9353475421667099,\n",
       " 'D_accuracy': 0.5634014457464218,\n",
       " 'ep_lens': [10,\n",
       "  4,\n",
       "  4,\n",
       "  11,\n",
       "  8,\n",
       "  22,\n",
       "  16,\n",
       "  6,\n",
       "  20,\n",
       "  29,\n",
       "  30,\n",
       "  13,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  17,\n",
       "  14,\n",
       "  12,\n",
       "  5,\n",
       "  6,\n",
       "  12,\n",
       "  3,\n",
       "  30,\n",
       "  22,\n",
       "  29,\n",
       "  30,\n",
       "  7,\n",
       "  23,\n",
       "  17],\n",
       " 'n_steps': 416,\n",
       " 'n_episodes': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = one_training_round(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discriminator,\n",
    "    actor_optim,\n",
    "    critic_optim,\n",
    "    discriminator_optim,\n",
    "    encode,\n",
    "    num_actions,\n",
    "    X_e,\n",
    "    expert_records=None,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ppo_clip=0.2,\n",
    "    epochs=4,\n",
    "    minibatch_size=256,\n",
    "    entropy_coeff=2e-2,\n",
    "    value_coeff=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    normalize_adv=True,\n",
    "    loss_type='bce',\n",
    "    gp_lambda=10.0,\n",
    "    d_updates=2,\n",
    "    d_minibatch_size=256,\n",
    "    use_gp=False,\n",
    "    instance_noise_std=0.05,\n",
    "    label_smoothing=0.0,\n",
    "    max_steps=num_steps,\n",
    "    num_episodes=train_eps,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005] R_env(m)=218.446  R_D(m)=0.689  π: L_actor=-0.016  V: L_critic=1.963  D: L=9.965  acc=0.604\n",
      "[010] R_env(m)=306.771  R_D(m)=0.691  π: L_actor=-0.021  V: L_critic=0.689  D: L=5.970  acc=0.614\n",
      "[015] R_env(m)=374.548  R_D(m)=0.693  π: L_actor=-0.006  V: L_critic=0.417  D: L=1.481  acc=0.614\n",
      "[020] R_env(m)=406.108  R_D(m)=0.669  π: L_actor=-0.008  V: L_critic=0.450  D: L=1.364  acc=0.814\n",
      "[025] R_env(m)=478.097  R_D(m)=0.627  π: L_actor=-0.009  V: L_critic=0.260  D: L=1.306  acc=0.925\n",
      "[030] R_env(m)=500.146  R_D(m)=0.577  π: L_actor=-0.018  V: L_critic=0.203  D: L=1.231  acc=0.952\n",
      "[035] R_env(m)=503.076  R_D(m)=0.511  π: L_actor=-0.001  V: L_critic=0.293  D: L=1.137  acc=0.970\n",
      "[040] R_env(m)=508.143  R_D(m)=0.452  π: L_actor=-0.005  V: L_critic=0.111  D: L=1.013  acc=0.981\n",
      "[045] R_env(m)=508.491  R_D(m)=0.407  π: L_actor=-0.010  V: L_critic=0.184  D: L=0.967  acc=0.980\n",
      "[050] R_env(m)=488.536  R_D(m)=0.365  π: L_actor=-0.023  V: L_critic=0.191  D: L=0.949  acc=0.980\n",
      "[055] R_env(m)=462.367  R_D(m)=0.337  π: L_actor=-0.033  V: L_critic=0.226  D: L=0.925  acc=0.981\n",
      "[060] R_env(m)=460.435  R_D(m)=0.322  π: L_actor=-0.015  V: L_critic=0.179  D: L=0.913  acc=0.969\n",
      "[065] R_env(m)=445.578  R_D(m)=0.335  π: L_actor=0.039  V: L_critic=0.513  D: L=1.028  acc=0.894\n",
      "[070] R_env(m)=441.867  R_D(m)=0.362  π: L_actor=-0.067  V: L_critic=0.727  D: L=1.081  acc=0.883\n",
      "[075] R_env(m)=408.970  R_D(m)=0.430  π: L_actor=-0.006  V: L_critic=0.997  D: L=1.258  acc=0.721\n",
      "[080] R_env(m)=355.117  R_D(m)=0.501  π: L_actor=0.036  V: L_critic=0.748  D: L=1.177  acc=0.804\n",
      "[085] R_env(m)=313.055  R_D(m)=0.547  π: L_actor=-0.004  V: L_critic=0.796  D: L=1.201  acc=0.755\n",
      "[090] R_env(m)=278.844  R_D(m)=0.585  π: L_actor=-0.004  V: L_critic=1.233  D: L=1.290  acc=0.659\n",
      "[095] R_env(m)=284.268  R_D(m)=0.568  π: L_actor=-0.003  V: L_critic=0.773  D: L=1.185  acc=0.772\n",
      "[100] R_env(m)=287.014  R_D(m)=0.556  π: L_actor=-0.004  V: L_critic=0.580  D: L=1.170  acc=0.802\n",
      "[105] R_env(m)=283.818  R_D(m)=0.545  π: L_actor=-0.004  V: L_critic=0.691  D: L=1.171  acc=0.799\n",
      "[110] R_env(m)=298.248  R_D(m)=0.530  π: L_actor=-0.003  V: L_critic=0.990  D: L=1.229  acc=0.748\n",
      "[115] R_env(m)=288.257  R_D(m)=0.537  π: L_actor=-0.006  V: L_critic=0.810  D: L=1.206  acc=0.768\n",
      "[120] R_env(m)=281.165  R_D(m)=0.541  π: L_actor=-0.002  V: L_critic=0.780  D: L=1.216  acc=0.756\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "log_every = 5\n",
    "ret_ma = deque(maxlen=20)\n",
    "dret_ma = deque(maxlen=20)\n",
    "\n",
    "for it in range(1, epochs + 1):\n",
    "    stats = one_training_round(\n",
    "        env,\n",
    "        actor,\n",
    "        critic,\n",
    "        discriminator,\n",
    "        actor_optim,\n",
    "        critic_optim,\n",
    "        discriminator_optim,\n",
    "        encode,\n",
    "        num_actions,\n",
    "        X_e,\n",
    "        expert_records=None,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ppo_clip=0.2,\n",
    "        epochs=4,\n",
    "        minibatch_size=256,\n",
    "        entropy_coeff=2e-2,\n",
    "        value_coeff=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_adv=True,\n",
    "        loss_type='bce',\n",
    "        gp_lambda=10.0,\n",
    "        d_updates=2,\n",
    "        d_minibatch_size=256,\n",
    "        use_gp=False,\n",
    "        instance_noise_std=0.05,\n",
    "        label_smoothing=0.0,\n",
    "        max_steps=num_steps,\n",
    "        num_episodes=train_eps,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    ret_ma.append(stats['avg_env_return'])\n",
    "    dret_ma.append(stats['avg_D_reward'])\n",
    "\n",
    "    if it % log_every == 0:\n",
    "        print(\n",
    "            f\"[{it:03d}] \"\n",
    "            f\"R_env(m)={np.mean(ret_ma):.3f}  \"\n",
    "            f\"R_D(m)={np.mean(dret_ma):.3f}  \"\n",
    "            f\"pi: L_actor={stats['ppo_actor_loss']:.3f}  \"\n",
    "            f\"V: L_critic={stats['ppo_critic_loss']:.3f}  \"\n",
    "            f\"D: L={stats['D_loss']:.3f}  acc={stats['D_accuracy']:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb26b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_policy(env, actor, encode, num_episodes=1000, max_steps=num_steps, seed=None):\n",
    "    actor.eval()\n",
    "    returns = []\n",
    "\n",
    "    for e in range(num_episodes):\n",
    "        rs = None if seed is None else seed + e + 1000\n",
    "        obs, _ = env.reset(seed=rs)\n",
    "        t, done, ret = 0, False, 0.0\n",
    "\n",
    "        while not done and t < max_steps:\n",
    "            z = torch.from_numpy(encode(obs, t)).float().unsqueeze(0).to(next(actor.parameters()).device)\n",
    "            a, _, _ = actor.act(z, deterministic=True)\n",
    "            obs, r, terminated, truncated, _ = env.do(lambda _: int(a.item()), show_reward=True)\n",
    "\n",
    "            ret += r\n",
    "            t += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        returns.append(ret)\n",
    "\n",
    "    return float(np.mean(returns)), returns\n",
    "\n",
    "_, imitator_rewards = eval_policy(env, actor, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert rewards: mean=676.280, std=205.085\n",
      "Imitator rewards: mean=270.594, std=264.848\n"
     ]
    }
   ],
   "source": [
    "expert_rewards = []\n",
    "reward = 0.0\n",
    "last_ep = records[0]['episode']\n",
    "\n",
    "for r in records:\n",
    "    if r['episode'] != last_ep:\n",
    "        expert_rewards.append(reward)\n",
    "        reward = 0.0\n",
    "        last_ep = r['episode']\n",
    "\n",
    "    reward += r['reward']\n",
    "\n",
    "print(f'Expert rewards: mean={np.mean(expert_rewards):.3f}, std={np.std(expert_rewards):.3f}')\n",
    "print(f'Imitator rewards: mean={np.mean(imitator_rewards):.3f}, std={np.std(imitator_rewards):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
