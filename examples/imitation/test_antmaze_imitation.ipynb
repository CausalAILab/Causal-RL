{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from causal_gym import AntMazePCH\n",
    "from causal_rl.algo.imitation.imitate import *\n",
    "from causal_rl.algo.imitation.finetune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "seed = 0\n",
    "lookback = 1\n",
    "hidden_dims = {'O'}\n",
    "train_eps = 1000\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_env = AntMazePCH(num_steps=num_steps, expert_mode=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AntMazePCH(num_steps=num_steps, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save time; conceptually the same\n",
    "small_steps = lookback + 1\n",
    "small_env = AntMazePCH(num_steps=small_steps, seed=seed)\n",
    "G = parse_graph(small_env.get_graph)\n",
    "X_small = {f'X{t}' for t in range(small_steps)}\n",
    "Y = f'Y{small_steps}'\n",
    "\n",
    "# G = parse_graph(env.get_graph)\n",
    "X = {f'X{t}' for t in range(num_steps)}\n",
    "# Y = f'Y{num_steps}'\n",
    "obs_prefix = env.env.observed_unobserved_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sets = find_sequential_pi_backdoor(G, X_small, Y, obs_prefix)\n",
    "\n",
    "base_step = small_steps - 1\n",
    "base_Z_set = Z_sets[f'X{base_step}']\n",
    "\n",
    "for i in range(base_step + 1, num_steps):\n",
    "    updated_base_Z_set = set()\n",
    "    for v in base_Z_set:\n",
    "        updated_base_Z_set.add(f'{v[0]}{int(v[1:]) + i - lookback}')\n",
    "\n",
    "    Z_sets[f'X{i}'] = updated_base_Z_set\n",
    "\n",
    "Z_sets['X1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c33186",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_Z_sets = {}\n",
    "for Xi in X:\n",
    "    i = int(Xi[1:])\n",
    "    cond = set()\n",
    "\n",
    "    for j in range(i+1):\n",
    "        cond.update({f'{o}{j}' for o in list(set(obs_prefix) - {'X'})})\n",
    "\n",
    "    for j in range(i):\n",
    "        cond.add(f'X{j}')\n",
    "    naive_Z_sets[Xi] = cond\n",
    "\n",
    "naive_Z_sets['X1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expert\n",
    "MODEL_PATH = '/home/et2842/causal/causalrl/models/antmaze_expert_finetuned.pt'\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "# Rebuild the model with the same architecture\n",
    "action_bounds = (checkpoint['action_bounds_low'], checkpoint['action_bounds_high'])\n",
    "\n",
    "expert = ContinuousPolicyNN(\n",
    "    input_dim=checkpoint['input_dim'],\n",
    "    action_dim=checkpoint['num_actions'],\n",
    "    hidden_dim=checkpoint['hidden_dim'],\n",
    "    num_blocks=checkpoint['num_blocks'],\n",
    "    dropout=checkpoint['dropout'],\n",
    "    layernorm=checkpoint['layernorm'],\n",
    "    final_tanh=checkpoint['final_tanh'],\n",
    "    action_bounds=action_bounds,\n",
    ").to(device)\n",
    "\n",
    "expert.load_state_dict(checkpoint['state_dict'])\n",
    "expert.eval()\n",
    "\n",
    "slots = checkpoint['slots']\n",
    "Z_trim = checkpoint['Z_trim']\n",
    "dims = checkpoint['dims']\n",
    "lookback = checkpoint['lookback']\n",
    "\n",
    "state_dim = checkpoint['input_dim']\n",
    "state_dim\n",
    "\n",
    "expert_policy = shared_policy_fn_long_horizon(expert, slots, Z_trim, continuous=True, device=device)\n",
    "expert_policies = make_shared_policy_dict(expert_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = collect_imitator_trajectories(\n",
    "    expert_env,\n",
    "    expert_policies,\n",
    "    num_episodes=train_eps,\n",
    "    max_steps=num_steps,\n",
    "    seed=seed,\n",
    "    hidden_dims=hidden_dims,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/et2842/causal/expert_traj.pkl', 'wb') as f:\n",
    "#     pickle.dump(records, f)\n",
    "\n",
    "# print(f'saved {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/et2842/causal/expert_traj.pkl', 'rb') as f:\n",
    "#     records = pickle.load(f)\n",
    "\n",
    "# print(f'loaded {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aa378",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "batch_size = 2048\n",
    "patience = 15\n",
    "num_blocks = 4\n",
    "epochs = 100\n",
    "dropout = 0.0\n",
    "\n",
    "dims = {\n",
    "    'P': 3,\n",
    "    # 'O': 4,\n",
    "    'A': 8,\n",
    "    'L': 3,\n",
    "    'T': 3,\n",
    "    'J': 8,\n",
    "    'W': 2,\n",
    "    'X': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_model, causal_slots, causal_Z_trim = train_single_policy_long_horizon(\n",
    "    records,\n",
    "    Z_sets,\n",
    "    dims=dims,\n",
    "    epochs=epochs,\n",
    "    include_vars=obs_prefix,\n",
    "    lookback=lookback,\n",
    "    continuous=True,\n",
    "    num_actions = env.action_space.shape[0],\n",
    "    hidden_dim=hidden_size,\n",
    "    num_blocks=num_blocks,\n",
    "    dropout=dropout,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    patience=patience,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    action_bounds=(env.action_space.low, env.action_space.high)\n",
    ")\n",
    "\n",
    "causal_policy = shared_policy_fn_long_horizon(causal_model, causal_slots, causal_Z_trim, continuous=True, device=device)\n",
    "causal_policies = make_shared_policy_dict(causal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model, naive_slots, naive_Z_trim = train_single_policy_long_horizon(\n",
    "    records,\n",
    "    naive_Z_sets,\n",
    "    dims=dims,\n",
    "    epochs=epochs,\n",
    "    include_vars=obs_prefix,\n",
    "    lookback=lookback,\n",
    "    continuous=True,\n",
    "    num_actions = env.action_space.shape[0],\n",
    "    hidden_dim=hidden_size,\n",
    "    num_blocks=num_blocks,\n",
    "    dropout=dropout,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    patience=patience,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    action_bounds=(env.action_space.low, env.action_space.high)\n",
    ")\n",
    "\n",
    "naive_policy = shared_policy_fn_long_horizon(naive_model, naive_slots, naive_Z_trim, continuous=True, device=device)\n",
    "naive_policies = make_shared_policy_dict(naive_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_episode_rewards = defaultdict(float)\n",
    "for rec in records:\n",
    "    ep = rec['episode']\n",
    "    expert_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "num_eps = len(expert_episode_rewards)\n",
    "expert_rewards = [expert_episode_rewards[e] for e in range(num_eps)]\n",
    "\n",
    "causal_records = collect_imitator_trajectories(\n",
    "    env,\n",
    "    causal_policies,\n",
    "    num_episodes=num_eps,\n",
    "    max_steps=num_steps,\n",
    "    hidden_dims=hidden_dims,\n",
    "    show_progress=True,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "causal_episode_rewards = defaultdict(float)\n",
    "for rec in causal_records:\n",
    "    ep = rec['episode']\n",
    "    causal_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "causal_rewards = [causal_episode_rewards[e] for e in range(num_eps)]\n",
    "\n",
    "naive_records = collect_imitator_trajectories(\n",
    "    env,\n",
    "    naive_policies,\n",
    "    num_episodes=num_eps,\n",
    "    max_steps=num_steps,\n",
    "    hidden_dims=hidden_dims,\n",
    "    show_progress=True,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "naive_episode_rewards = defaultdict(float)\n",
    "for rec in naive_records:\n",
    "    ep = rec['episode']\n",
    "    naive_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "naive_rewards = [naive_episode_rewards[e] for e in range(num_eps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9037b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute averages\n",
    "expert_avg = np.mean(expert_rewards)\n",
    "causal_avg = np.mean(causal_rewards)\n",
    "naive_avg  = np.mean(naive_rewards)\n",
    "\n",
    "# compute standard errors (optional but recommended)\n",
    "expert_std = np.std(expert_rewards)\n",
    "causal_std = np.std(causal_rewards)\n",
    "naive_std = np.std(naive_rewards)\n",
    "\n",
    "labels = ['Expert', 'Causal BC', 'Naive BC']\n",
    "averages = [expert_avg, causal_avg, naive_avg]\n",
    "errors = [expert_std, causal_std, naive_std]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(labels, averages, yerr=errors, capsize=6)\n",
    "plt.ylabel('Average Cumulative Reward')\n",
    "plt.title('Average Performance over Episodes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60967d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(records), len(causal_records), len(naive_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(expert_rewards)/num_eps, sum(causal_rewards)/num_eps, sum(naive_rewards)/num_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1acdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '/home/et2842/causal/causalrl/models'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, 'antmaze_causal_bc.pt')\n",
    "\n",
    "checkpoint = {\n",
    "    \"state_dict\": causal_model.state_dict(),\n",
    "    \"slots\": causal_slots,\n",
    "    \"Z_trim\": causal_Z_trim,\n",
    "    \"dims\": dims,\n",
    "    \"lookback\": lookback,\n",
    "    \"continuous\": True,\n",
    "    \"num_actions\": env.action_space.shape[0],\n",
    "    \"hidden_dim\": hidden_size,\n",
    "    \"num_blocks\": checkpoint['num_blocks'],\n",
    "    \"dropout\": 0.0,\n",
    "    \"layernorm\": True,\n",
    "    \"final_tanh\": True,\n",
    "    \"action_bounds_low\": env.action_space.low,\n",
    "    \"action_bounds_high\": env.action_space.high,\n",
    "    \"input_dim\": int(causal_model.hidden.in_features),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, MODEL_PATH)\n",
    "print(\"Saved expert to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '/home/et2842/causal/causalrl/models'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, 'antmaze_naive_bc.pt')\n",
    "\n",
    "checkpoint = {\n",
    "    \"state_dict\": naive_model.state_dict(),\n",
    "    \"slots\": naive_slots,\n",
    "    \"Z_trim\": naive_Z_trim,\n",
    "    \"dims\": dims,\n",
    "    \"lookback\": lookback,\n",
    "    \"continuous\": True,\n",
    "    \"num_actions\": env.action_space.shape[0],\n",
    "    \"hidden_dim\": hidden_size,\n",
    "    \"num_blocks\": checkpoint['num_blocks'],\n",
    "    \"dropout\": 0.0,\n",
    "    \"layernorm\": True,\n",
    "    \"final_tanh\": True,\n",
    "    \"action_bounds_low\": env.action_space.low,\n",
    "    \"action_bounds_high\": env.action_space.high,\n",
    "    \"input_dim\": int(naive_model.hidden.in_features),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, MODEL_PATH)\n",
    "print(\"Saved expert to:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib import cm\n",
    "\n",
    "def get_episode_xy_from_records(records, episode_id: int):\n",
    "    '''\n",
    "    records: list of dicts from collect_expert_trajectories(...)\n",
    "    episode_id: which episode to extract\n",
    "\n",
    "    Returns:\n",
    "        xs, ys : np.ndarray of shape (T,)\n",
    "    '''\n",
    "    # Filter records for that episode, sorted by step\n",
    "    ep = [r for r in records if r['episode'] == episode_id]\n",
    "    ep = sorted(ep, key=lambda r: r['step'])\n",
    "\n",
    "    xs, ys = [], []\n",
    "    for r in ep:\n",
    "        # r['info']['hidden_obs']['P'] is a *history* list; last entry is current position\n",
    "        pos = r['obs']['P'][-1]   # shape (3,)\n",
    "        xs.append(pos[0])\n",
    "        ys.append(pos[1])\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def plot_ant_trajectory_xy(records, episode_id: int = 0, ax=None, title_prefix='AntMaze'):\n",
    "    '''\n",
    "    Visualize the ant's 2D trajectory (x, y) for a single episode.\n",
    "\n",
    "    - Path is colored by time (early=dark, late=bright).\n",
    "    - Start and end are annotated.\n",
    "    - Small arrows show direction every few steps.\n",
    "    '''\n",
    "    xs, ys = get_episode_xy_from_records(records, episode_id)\n",
    "    T = len(xs)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    # Build a colored line collection for the path\n",
    "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "    # Time as color (0..1)\n",
    "    t_norm = np.linspace(0, 1, T-1)\n",
    "    lc = LineCollection(segments, cmap='viridis', norm=plt.Normalize(0, 1))\n",
    "    lc.set_array(t_norm)\n",
    "    lc.set_linewidth(2.5)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    # Start and end markers\n",
    "    ax.scatter(xs[0], xs[0], alpha=0)  # dummy to keep colors aligned if needed\n",
    "    ax.scatter(xs[0], ys[0], s=80, c='green', marker='o', edgecolors='black', label='Start')\n",
    "    ax.scatter(xs[-1], ys[-1], s=80, c='red', marker='X', edgecolors='black', label='End')\n",
    "\n",
    "    # Small arrows every N steps to show direction\n",
    "    step = max(1, T // 30)  # about ~30 arrows max\n",
    "    for i in range(0, T-1, step):\n",
    "        dx = xs[i+1] - xs[i]\n",
    "        dy = ys[i+1] - ys[i]\n",
    "        ax.arrow(xs[i], ys[i], dx, dy,\n",
    "                 length_includes_head=True,\n",
    "                 head_width=0.2,\n",
    "                 head_length=0.4,\n",
    "                 alpha=0.6)\n",
    "\n",
    "    # Colorbar for time\n",
    "    cbar = fig.colorbar(lc, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Time (normalized)')\n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlabel('x position')\n",
    "    ax.set_ylabel('y position')\n",
    "    ax.set_title(f'{title_prefix} - Episode {episode_id} trajectory')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e036e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ant_trajectory_xy(records, episode_id=i % num_eps, title_prefix='Expert AntMaze')\n",
    "plt.show()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ce4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ant_trajectory_xy(causal_records, episode_id=i % num_eps, title_prefix='Causal AntMaze')\n",
    "plt.show()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68603908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ant_trajectory_xy(naive_records, episode_id=i % num_eps, title_prefix='Naive AntMaze')\n",
    "plt.show()\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d4fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== W DIAGNOSTIC: ENV-LEVEL CHECK ===\")\n",
    "\n",
    "# 1) ENV-LEVEL CHECK: is W actually responding to U and affecting reward?\n",
    "debug_episodes = 5\n",
    "max_debug_steps = min(num_steps, 300)\n",
    "\n",
    "debug_env = AntMazePCH(num_steps=num_steps, hidden_dims=hidden_dims, seed=seed + 123)\n",
    "\n",
    "U_norms, W_vals, L_norms, Y_vals = [], [], [], []\n",
    "\n",
    "for ep in range(debug_episodes):\n",
    "    obs, info = debug_env.reset(seed=seed + 123 + ep)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    t = 0\n",
    "\n",
    "    while not (terminated or truncated) and t < max_debug_steps:\n",
    "        # random action rollout just to probe the dynamics\n",
    "        action = debug_env.env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = debug_env.env.step(\n",
    "            action, history=True, show_reward=True\n",
    "        )\n",
    "\n",
    "        # pull latest values directly from SCM\n",
    "        u = np.asarray(debug_env.env._U[-1], dtype=np.float64)\n",
    "        w = float(debug_env.env.W[-1][0])\n",
    "        l = float(np.linalg.norm(debug_env.env.L[-1]))\n",
    "        y = float(debug_env.env._Y[-1])\n",
    "\n",
    "        U_norms.append(float(np.linalg.norm(u)))\n",
    "        W_vals.append(w)\n",
    "        L_norms.append(l)\n",
    "        Y_vals.append(y)\n",
    "\n",
    "        t += 1\n",
    "\n",
    "U_norms = np.asarray(U_norms)\n",
    "W_vals = np.asarray(W_vals)\n",
    "L_norms = np.asarray(L_norms)\n",
    "Y_vals = np.asarray(Y_vals)\n",
    "\n",
    "def safe_corr(x, y, name_x, name_y):\n",
    "    if len(x) < 2 or np.allclose(np.var(x), 0) or np.allclose(np.var(y), 0):\n",
    "        print(f\"  [WARN] Not enough variation to compute correlation({name_x}, {name_y})\")\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "print(f\"Collected {len(U_norms)} debug steps over {debug_episodes} episodes.\")\n",
    "print(\"Basic stats:\")\n",
    "print(f\"  ||U||: mean={U_norms.mean():.3f}, std={U_norms.std():.3f}, min={U_norms.min():.3f}, max={U_norms.max():.3f}\")\n",
    "print(f\"  W    : mean={W_vals.mean():.3f}, std={W_vals.std():.3f}, min={W_vals.min():.3f}, max={W_vals.max():.3f}\")\n",
    "print(f\"  ||L||: mean={L_norms.mean():.3f}, std={L_norms.std():.3f}, min={L_norms.min():.3f}, max={L_norms.max():.3f}\")\n",
    "print(f\"  Y    : mean={Y_vals.mean():.3f}, std={Y_vals.std():.3f}, min={Y_vals.min():.3f}, max={Y_vals.max():.3f}\")\n",
    "\n",
    "corr_W_U = safe_corr(W_vals, U_norms, \"W\", \"||U||\")\n",
    "corr_W_Y = safe_corr(W_vals, Y_vals, \"W\", \"Y\")\n",
    "corr_U_Y = safe_corr(U_norms, Y_vals, \"||U||\", \"Y\")\n",
    "\n",
    "print(\"Correlations (env-level):\")\n",
    "print(f\"  corr(W, ||U||) = {corr_W_U:.3f}\")\n",
    "print(f\"  corr(W, Y)     = {corr_W_Y:.3f}\")\n",
    "print(f\"  corr(||U||, Y) = {corr_U_Y:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(U_norms, W_vals, s=5)\n",
    "plt.xlabel(\"||U||\")\n",
    "plt.ylabel(\"W\")\n",
    "plt.title(\"W vs ||U|| (env-level)\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(W_vals, Y_vals, s=5)\n",
    "plt.xlabel(\"W\")\n",
    "plt.ylabel(\"Reward Y\")\n",
    "plt.title(\"Y vs W (env-level)\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(U_norms, Y_vals, s=5)\n",
    "plt.xlabel(\"||U||\")\n",
    "plt.ylabel(\"Reward Y\")\n",
    "plt.title(\"Y vs ||U|| (env-level)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== W DIAGNOSTIC: DATASET-LEVEL CHECK (records) ===\")\n",
    "\n",
    "# 2) DATASET-LEVEL CHECK: is W present in expert records and correlated with X / Y?\n",
    "# Try to infer observation key from the first record.\n",
    "obs_key_candidates = [\"obs\", \"observation\", \"state\"]\n",
    "obs_key = None\n",
    "if len(records) > 0:\n",
    "    sample_keys = list(records[0].keys())\n",
    "    for k in obs_key_candidates:\n",
    "        if k in sample_keys:\n",
    "            obs_key = k\n",
    "            break\n",
    "\n",
    "print(f\"Detected observation key in records: {obs_key}\")\n",
    "if obs_key is None:\n",
    "    print(\"  [WARN] Could not find 'obs' / 'observation' / 'state' key in records; skipping dataset-level W check.\")\n",
    "else:\n",
    "    Ws_rec, Xs_rec, Ys_rec = [], [], []\n",
    "\n",
    "    for rec in records:\n",
    "        obs = rec.get(obs_key, None)\n",
    "        if obs is None or not isinstance(obs, dict):\n",
    "            continue\n",
    "\n",
    "        # W may be stored as a vector or history; handle both.\n",
    "        if \"W\" not in obs:\n",
    "            continue\n",
    "\n",
    "        w_val = np.asarray(obs[\"W\"])\n",
    "        if w_val.ndim > 1:\n",
    "            # assume (T,1) or (history,1) and take last step\n",
    "            w_val = w_val[-1]\n",
    "        Ws_rec.append(float(np.squeeze(w_val)))\n",
    "\n",
    "        # X may be stored similarly; if present, we grab its norm\n",
    "        if \"X\" in obs:\n",
    "            x_val = np.asarray(obs[\"X\"])\n",
    "            if x_val.ndim > 1:\n",
    "                x_val = x_val[-1]\n",
    "            Xs_rec.append(float(np.linalg.norm(x_val)))\n",
    "        else:\n",
    "            Xs_rec.append(np.nan)\n",
    "\n",
    "        # reward per record is already being used above\n",
    "        Ys_rec.append(float(rec.get(\"reward\", 0.0)))\n",
    "\n",
    "    Ws_rec = np.asarray(Ws_rec, dtype=np.float64)\n",
    "    Xs_rec = np.asarray(Xs_rec, dtype=np.float64)\n",
    "    Ys_rec = np.asarray(Ys_rec, dtype=np.float64)\n",
    "\n",
    "    print(f\"Extracted {len(Ws_rec)} W values from records.\")\n",
    "\n",
    "    if len(Ws_rec) < 2:\n",
    "        print(\"  [WARN] Not enough W values in records to compute correlations.\")\n",
    "    else:\n",
    "        corr_W_X = safe_corr(Ws_rec[~np.isnan(Xs_rec)], Xs_rec[~np.isnan(Xs_rec)], \"W\", \"||X||\")\n",
    "        corr_W_Y_rec = safe_corr(Ws_rec, Ys_rec, \"W\", \"Y (records)\")\n",
    "\n",
    "        print(\"Correlations (records-level):\")\n",
    "        print(f\"  corr(W, ||X||) = {corr_W_X:.3f}\")\n",
    "        print(f\"  corr(W, Y)     = {corr_W_Y_rec:.3f}\")\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(Ws_rec[~np.isnan(Xs_rec)], Xs_rec[~np.isnan(Xs_rec)], s=5)\n",
    "        plt.xlabel(\"W\")\n",
    "        plt.ylabel(\"||X||\")\n",
    "        plt.title(\"||X|| vs W (records)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(Ws_rec, Ys_rec, s=5)\n",
    "        plt.xlabel(\"W\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Reward vs W (records)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n[Done] W diagnostics complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== W DIAGNOSTIC: ENV-LEVEL CHECK ===\")\n",
    "\n",
    "# 1) ENV-LEVEL CHECK: test if W tracks U and affects Y\n",
    "debug_episodes = 5\n",
    "max_debug_steps = min(num_steps, 300)\n",
    "\n",
    "debug_env = AntMazePCH(num_steps=num_steps, hidden_dims=hidden_dims, seed=seed + 123)\n",
    "\n",
    "U_norms, W_vals, Y_vals = [], [], []\n",
    "X_dims = [[] for _ in range(debug_env.env.action_space.shape[0])]  # 8 lists\n",
    "\n",
    "for ep in range(debug_episodes):\n",
    "    obs, info = debug_env.reset(seed=seed + 123 + ep)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    t = 0\n",
    "\n",
    "    while not (terminated or truncated) and t < max_debug_steps:\n",
    "        action = debug_env.env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = debug_env.env.step(\n",
    "            action, history=True, show_reward=True\n",
    "        )\n",
    "\n",
    "        u = np.asarray(debug_env.env._U[-1], dtype=np.float64)\n",
    "        w = float(debug_env.env.W[-1][0])\n",
    "        y = float(debug_env.env._Y[-1])\n",
    "\n",
    "        U_norms.append(float(np.linalg.norm(u)))\n",
    "        W_vals.append(w)\n",
    "        Y_vals.append(y)\n",
    "\n",
    "        for k in range(len(X_dims)):\n",
    "            X_dims[k].append(float(action[k]))\n",
    "\n",
    "        t += 1\n",
    "\n",
    "U_norms = np.asarray(U_norms)\n",
    "W_vals  = np.asarray(W_vals)\n",
    "Y_vals  = np.asarray(Y_vals)\n",
    "X_dims  = [np.asarray(arr) for arr in X_dims]\n",
    "\n",
    "def safe_corr(x, y):\n",
    "    if len(x) < 3 or np.var(x) == 0 or np.var(y) == 0:\n",
    "        return np.nan\n",
    "    return float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "print(f\"Collected {len(W_vals)} steps\")\n",
    "\n",
    "print(\"\\nEnv-level stats:\")\n",
    "print(f\"  ||U|| mean={U_norms.mean():.3f}, std={U_norms.std():.3f}\")\n",
    "print(f\"  W     mean={W_vals.mean():.3f}, std={W_vals.std():.3f}\")\n",
    "print(f\"  Y     mean={Y_vals.mean():.3f}, std={Y_vals.std():.3f}\")\n",
    "\n",
    "print(\"\\nEnv-level correlations:\")\n",
    "print(f\"  corr(W, ||U||) = {safe_corr(W_vals, U_norms):.3f}\")\n",
    "print(f\"  corr(W, Y)     = {safe_corr(W_vals, Y_vals):.3f}\")\n",
    "print(f\"  corr(||U||, Y) = {safe_corr(U_norms, Y_vals):.3f}\")\n",
    "\n",
    "# Plot W vs action dims\n",
    "plt.figure(figsize=(16, 10))\n",
    "for i in range(len(X_dims)):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.scatter(W_vals, X_dims[i], s=4)\n",
    "    plt.xlabel(\"W\")\n",
    "    plt.ylabel(f\"X[{i}]\")\n",
    "    plt.title(f\"W vs X[{i}]\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\n=== W DIAGNOSTIC: DATASET-LEVEL CHECK (records) ===\")\n",
    "\n",
    "# Attempt to identify observation key\n",
    "obs_key_candidates = [\"obs\", \"observation\", \"state\"]\n",
    "obs_key = None\n",
    "if len(records) > 0:\n",
    "    for k in obs_key_candidates:\n",
    "        if k in records[0]:\n",
    "            obs_key = k\n",
    "            break\n",
    "\n",
    "print(f\"Detected observation key: {obs_key}\")\n",
    "\n",
    "if obs_key is None:\n",
    "    print(\"  [WARN] Could not locate observation key in records — skipping dataset-level check.\")\n",
    "else:\n",
    "    Ws_rec = []\n",
    "    Ys_rec = []\n",
    "    Xdims_rec = [[] for _ in range(env.env.action_space.shape[0])]\n",
    "\n",
    "    for rec in records:\n",
    "        obs = rec.get(obs_key, None)\n",
    "        if obs is None or not isinstance(obs, dict):\n",
    "            continue\n",
    "\n",
    "        # W\n",
    "        if \"W\" not in obs:\n",
    "            continue\n",
    "        wv = np.asarray(obs[\"W\"])\n",
    "        wv = wv[-1] if wv.ndim > 1 else wv\n",
    "        Ws_rec.append(float(wv.squeeze()))\n",
    "\n",
    "        # X\n",
    "        if \"X\" in obs:\n",
    "            xv = np.asarray(obs[\"X\"])\n",
    "            xv = xv[-1] if xv.ndim > 1 else xv\n",
    "            for k in range(len(Xdims_rec)):\n",
    "                Xdims_rec[k].append(float(xv[k]))\n",
    "        else:\n",
    "            for k in range(len(Xdims_rec)):\n",
    "                Xdims_rec[k].append(np.nan)\n",
    "\n",
    "        # Reward\n",
    "        Ys_rec.append(float(rec.get(\"reward\", 0.0)))\n",
    "\n",
    "    Ws_rec = np.asarray(Ws_rec)\n",
    "    Ys_rec = np.asarray(Ys_rec)\n",
    "    Xdims_rec = [np.asarray(arr) for arr in Xdims_rec]\n",
    "\n",
    "    print(f\"Extracted {len(Ws_rec)} W values from records.\")\n",
    "\n",
    "    if len(Ws_rec) < 3:\n",
    "        print(\"  [WARN] Not enough variation for dataset-level statistics.\")\n",
    "    else:\n",
    "        print(\"\\nDataset-level correlations:\")\n",
    "\n",
    "        print(f\"  corr(W, Y) = {safe_corr(Ws_rec, Ys_rec):.3f}\")\n",
    "        for k in range(len(Xdims_rec)):\n",
    "            mask = ~np.isnan(Xdims_rec[k])\n",
    "            corr = safe_corr(Ws_rec[mask], Xdims_rec[k][mask])\n",
    "            print(f\"  corr(W, X[{k}]) = {corr:.3f}\")\n",
    "\n",
    "        # Plot W vs action dims in dataset\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        for i in range(len(Xdims_rec)):\n",
    "            plt.subplot(3, 3, i+1)\n",
    "            plt.scatter(Ws_rec, Xdims_rec[i], s=4)\n",
    "            plt.xlabel(\"W\")\n",
    "            plt.ylabel(f\"X[{i}]\")\n",
    "            plt.title(f\"W vs X[{i}] (records)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n[Done] W diagnostics complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== ACTION VARIATION UNDER W: NAIVE vs CAUSAL ===\")\n",
    "\n",
    "def detect_obs_key(records):\n",
    "    if not records:\n",
    "        return None\n",
    "    candidates = [\"obs\", \"observation\", \"state\"]\n",
    "    keys = records[0].keys()\n",
    "    for k in candidates:\n",
    "        if k in keys:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "obs_key_naive  = detect_obs_key(naive_records)\n",
    "obs_key_causal = detect_obs_key(causal_records)\n",
    "print(f\"Detected observation key (naive):  {obs_key_naive}\")\n",
    "print(f\"Detected observation key (causal): {obs_key_causal}\")\n",
    "\n",
    "if obs_key_naive is None or obs_key_causal is None:\n",
    "    print(\"[WARN] Could not detect observation keys in naive/causal records.\")\n",
    "else:\n",
    "    action_dim = env.env.action_space.shape[0]\n",
    "\n",
    "    def extract_W_X(records, obs_key, action_dim):\n",
    "        W_list = []\n",
    "        X_lists = [[] for _ in range(action_dim)]\n",
    "\n",
    "        for rec in records:\n",
    "            obs = rec.get(obs_key, None)\n",
    "            if obs is None or not isinstance(obs, dict):\n",
    "                continue\n",
    "\n",
    "            # W may be scalar or last element of history\n",
    "            if \"W\" not in obs:\n",
    "                continue\n",
    "            wv = np.asarray(obs[\"W\"])\n",
    "            if wv.ndim > 1:\n",
    "                wv = wv[-1]\n",
    "            W_list.append(float(np.squeeze(wv)))\n",
    "\n",
    "            # X may be vector or last element of history\n",
    "            if \"X\" in obs:\n",
    "                xv = np.asarray(obs[\"X\"])\n",
    "                if xv.ndim > 1:\n",
    "                    xv = xv[-1]\n",
    "                for k in range(action_dim):\n",
    "                    X_lists[k].append(float(xv[k]))\n",
    "            else:\n",
    "                # if X missing, pad with NaNs to keep lengths aligned\n",
    "                for k in range(action_dim):\n",
    "                    X_lists[k].append(np.nan)\n",
    "\n",
    "        W_arr = np.asarray(W_list, dtype=np.float64)\n",
    "        X_arrs = [np.asarray(xs, dtype=np.float64) for xs in X_lists]\n",
    "        return W_arr, X_arrs\n",
    "\n",
    "    W_naive,  X_naive_dims  = extract_W_X(naive_records,  obs_key_naive,  action_dim)\n",
    "    W_causal, X_causal_dims = extract_W_X(causal_records, obs_key_causal, action_dim)\n",
    "\n",
    "    print(f\"Naive:  extracted {len(W_naive)} (W, X) pairs.\")\n",
    "    print(f\"Causal: extracted {len(W_causal)} (W, X) pairs.\")\n",
    "\n",
    "    def safe_corr(x, y):\n",
    "        mask = ~np.isnan(y)\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        if len(x) < 3 or np.var(x) == 0 or np.var(y) == 0:\n",
    "            return np.nan\n",
    "        return float(np.corrcoef(x, y)[0, 1])\n",
    "\n",
    "    # Per-dimension summary\n",
    "    print(\"\\nPer-dimension correlations corr(W, X[d])\")\n",
    "    for d in range(action_dim):\n",
    "        corr_naive  = safe_corr(W_naive,  X_naive_dims[d])\n",
    "        corr_causal = safe_corr(W_causal, X_causal_dims[d])\n",
    "        print(f\"  dim {d}: naive={corr_naive:.3f}, causal={corr_causal:.3f}\")\n",
    "\n",
    "    # Plots: W vs X[d], naive vs causal\n",
    "    n_rows = int(np.ceil(action_dim / 3))\n",
    "    plt.figure(figsize=(16, 4 * n_rows))\n",
    "\n",
    "    for d in range(action_dim):\n",
    "        plt.subplot(n_rows, 3, d + 1)\n",
    "\n",
    "        # causal\n",
    "        mask_c = ~np.isnan(X_causal_dims[d])\n",
    "        plt.scatter(W_causal[mask_c], X_causal_dims[d][mask_c], s=4, alpha=0.5, label=\"Causal\")\n",
    "\n",
    "        # naive\n",
    "        mask_n = ~np.isnan(X_naive_dims[d])\n",
    "        plt.scatter(W_naive[mask_n], X_naive_dims[d][mask_n], s=4, alpha=0.5, label=\"Naive\")\n",
    "\n",
    "        plt.xlabel(\"W\")\n",
    "        plt.ylabel(f\"X[{d}]\")\n",
    "        plt.title(f\"W vs X[{d}]\")\n",
    "        if d == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n[Done] Action-W variation diagnostics complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c6300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== Spuriousness of new 2D W in expert records ===\")\n",
    "\n",
    "# 1) Detect observation key\n",
    "def detect_obs_key(records):\n",
    "    if not records:\n",
    "        return None\n",
    "    candidates = [\"obs\", \"observation\", \"state\"]\n",
    "    for k in candidates:\n",
    "        if k in records[0]:\n",
    "            return k\n",
    "    return None\n",
    "\n",
    "obs_key = detect_obs_key(records)\n",
    "print(f\"Detected observation key: {obs_key}\")\n",
    "if obs_key is None:\n",
    "    print(\"[WARN] Could not find obs key in records; aborting.\")\n",
    "else:\n",
    "    action_dim = env.env.action_space.shape[0]\n",
    "\n",
    "    W_list = []            # will become (N, 2)\n",
    "    X_list = []            # will become (N, action_dim)\n",
    "    Y_list = []            # rewards, optional\n",
    "\n",
    "    for rec in records:\n",
    "        obs = rec.get(obs_key, None)\n",
    "        if obs is None or not isinstance(obs, dict):\n",
    "            continue\n",
    "        if \"W\" not in obs or \"X\" not in obs:\n",
    "            continue\n",
    "\n",
    "        # W may be (2,) or (T,2); take last if history\n",
    "        wv = np.asarray(obs[\"W\"])\n",
    "        if wv.ndim > 1:\n",
    "            wv = wv[-1]\n",
    "        wv = np.asarray(wv).reshape(-1)\n",
    "        if wv.shape[0] != 2:\n",
    "            # skip weird shapes\n",
    "            continue\n",
    "\n",
    "        # X may be (8,) or (T,8); take last if history\n",
    "        xv = np.asarray(obs[\"X\"])\n",
    "        if xv.ndim > 1:\n",
    "            xv = xv[-1]\n",
    "        xv = np.asarray(xv).reshape(-1)\n",
    "        if xv.shape[0] != action_dim:\n",
    "            continue\n",
    "\n",
    "        W_list.append(wv)\n",
    "        X_list.append(xv)\n",
    "        Y_list.append(float(rec.get(\"reward\", 0.0)))\n",
    "\n",
    "    if len(W_list) == 0:\n",
    "        print(\"[WARN] No valid (W,X) pairs found in records.\")\n",
    "    else:\n",
    "        W_arr = np.stack(W_list, axis=0)        # (N, 2)\n",
    "        X_arr = np.stack(X_list, axis=0)        # (N, action_dim)\n",
    "        Y_arr = np.asarray(Y_list, dtype=np.float64)\n",
    "\n",
    "        print(f\"Collected {W_arr.shape[0]} expert steps with 2D W and {action_dim}D X.\")\n",
    "\n",
    "        def safe_corr(a, b):\n",
    "            if a.size < 3 or b.size < 3:\n",
    "                return np.nan\n",
    "            if np.allclose(np.var(a), 0) or np.allclose(np.var(b), 0):\n",
    "                return np.nan\n",
    "            return float(np.corrcoef(a, b)[0, 1])\n",
    "\n",
    "        # 2) Correlation matrix: W[d_w] with X[d_x]\n",
    "        print(\"\\nCorrelation matrix corr(W[d_w], X[d_x])\")\n",
    "        for d_w in range(2):\n",
    "            row = []\n",
    "            for d_x in range(action_dim):\n",
    "                c = safe_corr(W_arr[:, d_w], X_arr[:, d_x])\n",
    "                row.append(f\"{c: .3f}\")\n",
    "            print(f\"W[{d_w}] :\", \"  \".join(row))\n",
    "\n",
    "        # Optionally, also W vs reward\n",
    "        for d_w in range(2):\n",
    "            c = safe_corr(W_arr[:, d_w], Y_arr)\n",
    "            print(f\"corr(W[{d_w}], reward) = {c:.3f}\")\n",
    "\n",
    "        # 3) Plots: W[0] and W[1] vs each action dim\n",
    "        n_cols = 4\n",
    "        n_rows = int(np.ceil(action_dim / n_cols))\n",
    "\n",
    "        # W[0] vs X[*]\n",
    "        plt.figure(figsize=(4*n_cols, 3*n_rows))\n",
    "        for d in range(action_dim):\n",
    "            plt.subplot(n_rows, n_cols, d+1)\n",
    "            plt.scatter(W_arr[:, 0], X_arr[:, d], s=4, alpha=0.5)\n",
    "            plt.xlabel(\"W[0]\")\n",
    "            plt.ylabel(f\"X[{d}]\")\n",
    "            plt.title(f\"W[0] vs X[{d}] (expert)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # W[1] vs X[*]\n",
    "        plt.figure(figsize=(4*n_cols, 3*n_rows))\n",
    "        for d in range(action_dim):\n",
    "            plt.subplot(n_rows, n_cols, d+1)\n",
    "            plt.scatter(W_arr[:, 1], X_arr[:, d], s=4, alpha=0.5)\n",
    "            plt.xlabel(\"W[1]\")\n",
    "            plt.ylabel(f\"X[{d}]\")\n",
    "            plt.title(f\"W[1] vs X[{d}] (expert)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n[Done] 2D W–X spuriousness diagnostics on expert records.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
