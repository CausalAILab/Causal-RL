{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from causal_gym import AntMazePCH\n",
    "from causal_rl.algo.imitation.imitate import *\n",
    "from causal_rl.algo.imitation.finetune import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "seed = 0\n",
    "hidden_dims = {'F'}\n",
    "lookback = 1\n",
    "train_eps = 50\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_env = AntMazePCH(num_steps=num_steps, hidden_dims=set(), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa780b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AntMazePCH(num_steps=num_steps, hidden_dims=hidden_dims, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1e51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save time; conceptually the same\n",
    "small_steps = lookback + 1\n",
    "small_env = AntMazePCH(num_steps=small_steps, hidden_dims=hidden_dims, seed=seed)\n",
    "G = parse_graph(small_env.get_graph)\n",
    "X_small = {f'X{t}' for t in range(small_steps)}\n",
    "Y = f'Y{small_steps}'\n",
    "\n",
    "# G = parse_graph(env.get_graph)\n",
    "X = {f'X{t}' for t in range(num_steps)}\n",
    "# Y = f'Y{num_steps}'\n",
    "obs_prefix = env.env.observed_unobserved_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3324c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sets = find_sequential_pi_backdoor(G, X_small, Y, obs_prefix)\n",
    "\n",
    "base_step = small_steps - 1\n",
    "base_Z_set = Z_sets[f'X{base_step}']\n",
    "\n",
    "for i in range(base_step + 1, num_steps):\n",
    "    updated_base_Z_set = set()\n",
    "    for v in base_Z_set:\n",
    "        updated_base_Z_set.add(f'{v[0]}{int(v[1:]) + i - lookback}')\n",
    "\n",
    "    Z_sets[f'X{i}'] = updated_base_Z_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c33186",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_Z_sets = {}\n",
    "for Xi in X:\n",
    "    i = int(Xi[1:])\n",
    "    cond = set()\n",
    "\n",
    "    for j in range(i+1):\n",
    "        cond.update({f'{o}{j}' for o in list(set(obs_prefix) - {'X'})})\n",
    "\n",
    "    for j in range(i):\n",
    "        cond.add(f'X{j}')\n",
    "    naive_Z_sets[Xi] = cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expert\n",
    "MODEL_PATH = '/Users/eylam/Desktop/Causal Inference/causalrl/models/antmaze_expert_finetuned.pt'\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "\n",
    "# Rebuild the model with the same architecture\n",
    "action_bounds = (checkpoint['action_bounds_low'], checkpoint['action_bounds_high'])\n",
    "\n",
    "expert = ContinuousPolicyNN(\n",
    "    input_dim=checkpoint['input_dim'],\n",
    "    action_dim=checkpoint['num_actions'],\n",
    "    hidden_dim=checkpoint['hidden_dim'],\n",
    "    num_blocks=checkpoint['num_blocks'],\n",
    "    dropout=checkpoint['dropout'],\n",
    "    layernorm=checkpoint['layernorm'],\n",
    "    final_tanh=checkpoint['final_tanh'],\n",
    "    action_bounds=action_bounds,\n",
    ").to(device)\n",
    "\n",
    "expert.load_state_dict(checkpoint['state_dict'])\n",
    "expert.eval()\n",
    "\n",
    "slots = checkpoint['slots']\n",
    "Z_trim = checkpoint['Z_trim']\n",
    "dims = checkpoint['dims']\n",
    "lookback = checkpoint['lookback']\n",
    "\n",
    "state_dim = checkpoint['input_dim']\n",
    "state_dim\n",
    "\n",
    "def make_ft_policies_with_F(model, slots, Z_trim, device, num_steps, extra_vars):\n",
    "    policies = {}\n",
    "    for t in range(num_steps):\n",
    "        def pi_t(obs, t=t):\n",
    "            state = build_state_feature(obs, t, Z_trim, slots, device, extra_vars=extra_vars)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                action = model(state).squeeze(0).cpu().numpy().astype(np.float32)\n",
    "\n",
    "            return action\n",
    "        \n",
    "        policies[f'X{t}'] = pi_t\n",
    "\n",
    "    return policies\n",
    "\n",
    "expert_policies = make_ft_policies_with_F(\n",
    "    expert,\n",
    "    slots=slots,\n",
    "    Z_trim=Z_trim,\n",
    "    device=device,\n",
    "    num_steps=num_steps,\n",
    "    extra_vars={'F'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = collect_imitator_trajectories(\n",
    "    expert_env,\n",
    "    expert_policies,\n",
    "    num_episodes=train_eps,\n",
    "    max_steps=num_steps,\n",
    "    seed=seed,\n",
    "    hidden_dims=hidden_dims,\n",
    "    lookback=lookback,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f573b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/eylam/Desktop/Causal Inference/expert_traj.pkl', 'wb') as f:\n",
    "#     pickle.dump(records, f)\n",
    "\n",
    "# print(f'saved {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81bc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/et2842/causal/expert_traj.pkl', 'rb') as f:\n",
    "#     records = pickle.load(f)\n",
    "\n",
    "# print(f'loaded {len(records)} trajectories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aa378",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "lr = 3e-4\n",
    "batch_size = 2048\n",
    "patience = 15\n",
    "num_blocks = 4\n",
    "epochs = 100\n",
    "dropout = 0.0\n",
    "\n",
    "dims = {\n",
    "    'P': 3,\n",
    "    'O': 4,\n",
    "    'A': 8,\n",
    "    'L': 3,\n",
    "    'T': 3,\n",
    "    'J': 8,\n",
    "    # 'F': 2,\n",
    "    'W': 1,\n",
    "    'X': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb5bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_model, causal_slots, causal_Z_trim = train_single_policy_long_horizon(\n",
    "    records,\n",
    "    Z_sets,\n",
    "    dims=dims,\n",
    "    epochs=epochs,\n",
    "    include_vars=obs_prefix,\n",
    "    lookback=lookback,\n",
    "    continuous=True,\n",
    "    num_actions = env.action_space.shape[0],\n",
    "    hidden_dim=hidden_size,\n",
    "    num_blocks=num_blocks,\n",
    "    dropout=dropout,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    patience=patience,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    action_bounds=(env.action_space.low, env.action_space.high)\n",
    ")\n",
    "\n",
    "causal_policy = shared_policy_fn_long_horizon(causal_model, causal_slots, causal_Z_trim, continuous=True, device=device)\n",
    "causal_policies = make_shared_policy_dict(causal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model, naive_slots, naive_Z_trim = train_single_policy_long_horizon(\n",
    "    records,\n",
    "    naive_Z_sets,\n",
    "    dims=dims,\n",
    "    epochs=epochs,\n",
    "    include_vars=obs_prefix,\n",
    "    lookback=lookback,\n",
    "    continuous=True,\n",
    "    num_actions = env.action_space.shape[0],\n",
    "    hidden_dim=hidden_size,\n",
    "    num_blocks=num_blocks,\n",
    "    dropout=dropout,\n",
    "    lr=lr,\n",
    "    batch_size=batch_size,\n",
    "    patience=patience,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    action_bounds=(env.action_space.low, env.action_space.high)\n",
    ")\n",
    "\n",
    "naive_policy = shared_policy_fn_long_horizon(naive_model, naive_slots, naive_Z_trim, continuous=True, device=device)\n",
    "naive_policies = make_shared_policy_dict(naive_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dfb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_episode_rewards = defaultdict(float)\n",
    "for rec in records:\n",
    "    ep = rec['episode']\n",
    "    expert_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "num_eps = len(expert_episode_rewards)\n",
    "expert_rewards = [expert_episode_rewards[e] for e in range(num_eps)]\n",
    "\n",
    "causal_records = collect_imitator_trajectories(env, causal_policies, num_episodes=num_eps, max_steps=num_steps, seed=seed)\n",
    "causal_episode_rewards = defaultdict(float)\n",
    "for rec in causal_records:\n",
    "    ep = rec['episode']\n",
    "    causal_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "causal_rewards = [causal_episode_rewards[e] for e in range(num_eps)]\n",
    "\n",
    "naive_records = collect_imitator_trajectories(env, naive_policies, num_episodes=num_eps, max_steps=num_steps, seed=seed)\n",
    "naive_episode_rewards = defaultdict(float)\n",
    "for rec in naive_records:\n",
    "    ep = rec['episode']\n",
    "    naive_episode_rewards[ep] += float(rec['reward'])\n",
    "\n",
    "naive_rewards = [naive_episode_rewards[e] for e in range(num_eps)]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(expert_rewards, label='Expert')\n",
    "plt.plot(causal_rewards, label='Causal BC')\n",
    "plt.plot(naive_rewards, label='Naive BC')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Final Cumulative Reward')\n",
    "plt.title('Comparison of Expert vs. Causal vs. Naive Returns')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(expert_rewards)/num_eps, sum(causal_rewards)/num_eps, sum(naive_rewards)/num_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib import cm\n",
    "\n",
    "def get_episode_xy_from_records(records, episode_id: int):\n",
    "    '''\n",
    "    records: list of dicts from collect_expert_trajectories(...)\n",
    "    episode_id: which episode to extract\n",
    "\n",
    "    Returns:\n",
    "        xs, ys : np.ndarray of shape (T,)\n",
    "    '''\n",
    "    # Filter records for that episode, sorted by step\n",
    "    ep = [r for r in records if r['episode'] == episode_id]\n",
    "    ep = sorted(ep, key=lambda r: r['step'])\n",
    "\n",
    "    xs, ys = [], []\n",
    "    for r in ep:\n",
    "        # r['info']['hidden_obs']['P'] is a *history* list; last entry is current position\n",
    "        pos = r['obs']['P'][-1]   # shape (3,)\n",
    "        xs.append(pos[0])\n",
    "        ys.append(pos[1])\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def plot_ant_trajectory_xy(records, episode_id: int = 0, ax=None, title_prefix='AntMaze'):\n",
    "    '''\n",
    "    Visualize the ant's 2D trajectory (x, y) for a single episode.\n",
    "\n",
    "    - Path is colored by time (early=dark, late=bright).\n",
    "    - Start and end are annotated.\n",
    "    - Small arrows show direction every few steps.\n",
    "    '''\n",
    "    xs, ys = get_episode_xy_from_records(records, episode_id)\n",
    "    T = len(xs)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "\n",
    "    # Build a colored line collection for the path\n",
    "    points = np.array([xs, ys]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "\n",
    "    # Time as color (0..1)\n",
    "    t_norm = np.linspace(0, 1, T-1)\n",
    "    lc = LineCollection(segments, cmap='viridis', norm=plt.Normalize(0, 1))\n",
    "    lc.set_array(t_norm)\n",
    "    lc.set_linewidth(2.5)\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    # Start and end markers\n",
    "    ax.scatter(xs[0], xs[0], alpha=0)  # dummy to keep colors aligned if needed\n",
    "    ax.scatter(xs[0], ys[0], s=80, c='green', marker='o', edgecolors='black', label='Start')\n",
    "    ax.scatter(xs[-1], ys[-1], s=80, c='red', marker='X', edgecolors='black', label='End')\n",
    "\n",
    "    # Small arrows every N steps to show direction\n",
    "    step = max(1, T // 30)  # about ~30 arrows max\n",
    "    for i in range(0, T-1, step):\n",
    "        dx = xs[i+1] - xs[i]\n",
    "        dy = ys[i+1] - ys[i]\n",
    "        ax.arrow(xs[i], ys[i], dx, dy,\n",
    "                 length_includes_head=True,\n",
    "                 head_width=0.2,\n",
    "                 head_length=0.4,\n",
    "                 alpha=0.6)\n",
    "\n",
    "    # Colorbar for time\n",
    "    cbar = fig.colorbar(lc, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Time (normalized)')\n",
    "\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.set_xlabel('x position')\n",
    "    ax.set_ylabel('y position')\n",
    "    ax.set_title(f'{title_prefix} - Episode {episode_id} trajectory')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_ant_trajectory_xy(records, episode_id=0, title_prefix='Expert AntMaze')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ce4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ant_trajectory_xy(causal_records, episode_id=0, title_prefix='Causal AntMaze')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68603908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ant_trajectory_xy(naive_records, episode_id=0, title_prefix='Naive AntMaze')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
