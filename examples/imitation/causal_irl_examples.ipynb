{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07bd294",
   "metadata": {},
   "source": [
    "# Causal Imitation Learning Examples - Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1fc7aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 100000\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# utility\n",
    "def logistic(size=None):\n",
    "    return rng.logistic(loc=0.0, scale=1.0, size=size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6b7c2",
   "metadata": {},
   "source": [
    "### 9.24: Inverse RL in MAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf94092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert rollouts (NUC holds, expert copies U)\n",
    "p = 0.9\n",
    "theta = 2.0\n",
    "N_expert = N\n",
    "U_expert = rng.binomial(1, p, size=N_expert)\n",
    "X_expert = U_expert.copy()\n",
    "Y_expert = X_expert.copy()\n",
    "R_expert = theta * Y_expert\n",
    "p_hat = Y_expert.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfcbaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC baseline\n",
    "N_eval = N\n",
    "q_bc = p_hat\n",
    "X_bc = rng.binomial(1, q_bc, size=N_eval)\n",
    "Y_bc = X_bc.copy()\n",
    "R_bc = theta * Y_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRL minimax (NUC holds)\n",
    "q_star = 1.0\n",
    "\n",
    "X_irl = np.ones(N_eval, dtype=int)\n",
    "Y_irl = X_irl.copy()\n",
    "R_irl = theta * Y_irl\n",
    "\n",
    "# show worst-case gap\n",
    "def worst_case_gap(q, p_expert):\n",
    "    return max(0.0, p_expert - q)\n",
    "\n",
    "grid_q = np.linspace(0, 1, 6)\n",
    "gaps = [worst_case_gap(q, p_hat) for q in grid_q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ddb2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRL minimax in 1-step MAB (NUC holds):\n",
      "  U ~ Bern(p), X := U, Y := X, Rθ(Y) = θ * Y (θ > 0)\n",
      "  Demo p = P(U=1) = E_demo[Y] = 0.900 (empirical 0.90095)\n",
      "\n",
      "Behavioral Cloning (baseline):\n",
      "  q_bc := π_BC(X=1) = P_demo(X=1) = 0.90095\n",
      "  Value_BC = θ * q_bc = 1.80190 (empirical 1.80478)\n",
      "\n",
      "IRL (minimax) solution:\n",
      "  Choose q* := π*(X=1) = 1.0  (any q ≥ p makes the worst-case gap zero; choosing 1 is canonical)\n",
      "  Value_IRL = θ * 1 = 2.00000 (empirical 2.00000)\n",
      "\n",
      "Comparison:\n",
      "  Expert value = θ * p = 1.80000\n",
      "  BC value     = θ * p = 1.80190\n",
      "  IRL value    = θ * 1 = 2.00000\n",
      "  IRL improves over expert/BC by: 0.20000\n",
      "\n",
      "Worst-case gap max_{θ>0} θ*(p - q) (up to positive scaling):\n",
      "  q=0.0 -> gap ~ 0.901\n",
      "  q=0.2 -> gap ~ 0.701\n",
      "  q=0.4 -> gap ~ 0.501\n",
      "  q=0.6 -> gap ~ 0.301\n",
      "  q=0.8 -> gap ~ 0.101\n",
      "  q=1.0 -> gap ~ 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"IRL minimax in 1-step MAB (NUC holds):\")\n",
    "print(\"  U ~ Bern(p), X := U, Y := X, Rθ(Y) = θ * Y (θ > 0)\")\n",
    "print(f\"  Demo p = P(U=1) = E_demo[Y] = {p:.3f} (empirical {p_hat:.5f})\")\n",
    "print()\n",
    "print(\"Behavioral Cloning (baseline):\")\n",
    "print(f\"  q_bc := π_BC(X=1) = P_demo(X=1) = {q_bc:.5f}\")\n",
    "print(f\"  Value_BC = θ * q_bc = {theta*q_bc:.5f} (empirical {R_bc.mean():.5f})\")\n",
    "print()\n",
    "print(\"IRL (minimax) solution:\")\n",
    "print(\"  Choose q* := π*(X=1) = 1.0  (any q ≥ p makes the worst-case gap zero; choosing 1 is canonical)\")\n",
    "print(f\"  Value_IRL = θ * 1 = {theta*1.0:.5f} (empirical {R_irl.mean():.5f})\")\n",
    "print()\n",
    "print(\"Comparison:\")\n",
    "print(f\"  Expert value = θ * p = {theta*p:.5f}\")\n",
    "print(f\"  BC value     = θ * p = {theta*q_bc:.5f}\")\n",
    "print(f\"  IRL value    = θ * 1 = {theta*1.0:.5f}\")\n",
    "print(f\"  IRL improves over expert/BC by: {theta*(1.0 - p):.5f}\")\n",
    "print()\n",
    "print(\"Worst-case gap max_{θ>0} θ*(p - q) (up to positive scaling):\")\n",
    "for q, g in zip(grid_q, gaps):\n",
    "    print(f\"  q={q:.1f} -> gap ~ {g:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ac954",
   "metadata": {},
   "source": [
    "### 9.25: Inverse RL fails without NUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7c7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2.0\n",
    "N_expert = N\n",
    "N_eval = N\n",
    "\n",
    "def xor(a, b):\n",
    "    return (a ^ b).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0f4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_expert = rng.binomial(1, 0.5, size=N_expert)\n",
    "X_expert = 1 - U_expert\n",
    "Y_expert = xor(X_expert, U_expert)\n",
    "\n",
    "Ey_x0 = Y_expert[X_expert == 0].mean()\n",
    "Ey_x1 = Y_expert[X_expert == 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c330ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Off-Policy Evaluation\n",
    "def naive_ope_value(q, Ey_x0, Ey_x1):\n",
    "    return (1 - q) * Ey_x0 + q * Ey_x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4c2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_star = 1.0\n",
    "V_hat_irl = theta * naive_ope_value(q_star, Ey_x0, Ey_x1)\n",
    "\n",
    "# baseline\n",
    "q_bc = X_expert.mean()\n",
    "V_hat_bc = theta * naive_ope_value(q_bc, Ey_x0, Ey_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "175ff83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# irl rollout\n",
    "U_eval = rng.binomial(1, 0.5, size=N_eval)\n",
    "X_irl = np.ones(N_eval, dtype=int)\n",
    "Y_irl = xor(X_irl, U_eval)\n",
    "R_irl = theta * Y_irl\n",
    "V_true_irl = R_irl.mean()\n",
    "\n",
    "# expert rollout\n",
    "X_expert = 1 - U_eval\n",
    "Y_expert = xor(X_expert, U_eval)\n",
    "V_true_expert = (theta * Y_expert).mean()\n",
    "\n",
    "# bc rollout\n",
    "X_bc = rng.binomial(1, q_bc, size=N_eval)\n",
    "Y_bc = xor(X_bc, U_eval)\n",
    "V_true_bc = (theta * Y_bc).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad6e8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCM (confounded XOR MAB):\n",
      "  U ~ Bern(0.5)\n",
      "  Expert: X := NOT U\n",
      "  Outcome: Y := XOR(X, U)\n",
      "  Reward: R = θ * Y  (θ > 0)\n",
      "\n",
      "From expert (observational) data:\n",
      "  E[Y|X=0] ≈ 1.00000,  E[Y|X=1] ≈ 1.00000  (both ~1 ⇒ naïve off-policy eval believes any policy gets θ)\n",
      "  E_demo[Y] ≈ 1.00000 (should be ~1)\n",
      "\n",
      "Naive Off-policy Evaluation (WRONG, assumes NUC):\n",
      "  Predicted IRL value for q* = 1.0:  V_hat(π*) = 2.00000\n",
      "  Predicted BC value  for q_bc=0.50:  V_hat(BC) = 2.00000\n",
      "\n",
      "TRUE deployment on the SCM:\n",
      "  Expert true value         = 2.00000   (θ * 1)\n",
      "  IRL(π*: X≡1) true value   = 0.99728   (≈ θ * 0.5)\n",
      "  BC  true value            = 1.00256   (≈ θ * 0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SCM (confounded XOR MAB):\")\n",
    "print(\"  U ~ Bern(0.5)\")\n",
    "print(\"  Expert: X := NOT U\")\n",
    "print(\"  Outcome: Y := XOR(X, U)\")\n",
    "print(\"  Reward: R = θ * Y  (θ > 0)\\n\")\n",
    "\n",
    "print(\"From expert (observational) data:\")\n",
    "print(f\"  E[Y|X=0] ≈ {Ey_x0:.5f},  E[Y|X=1] ≈ {Ey_x1:.5f}  (both ~1 ⇒ naïve off-policy eval believes any policy gets θ)\")\n",
    "print(f\"  E_demo[Y] ≈ {Y_expert.mean():.5f} (should be ~1)\\n\")\n",
    "\n",
    "print(\"Naive Off-policy Evaluation (WRONG, assumes NUC):\")\n",
    "print(f\"  Predicted IRL value for q* = 1.0:  V_hat(π*) = {V_hat_irl:.5f}\")\n",
    "print(f\"  Predicted BC value  for q_bc={q_bc:.2f}:  V_hat(BC) = {V_hat_bc:.5f}\\n\")\n",
    "\n",
    "print(\"TRUE deployment on the SCM:\")\n",
    "print(f\"  Expert true value         = {V_true_expert:.5f}   (θ * 1)\")\n",
    "print(f\"  IRL(π*: X≡1) true value   = {V_true_irl:.5f}   (≈ θ * 0.5)\")\n",
    "print(f\"  BC  true value            = {V_true_bc:.5f}   (≈ θ * 0.5)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
