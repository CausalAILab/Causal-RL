{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc39db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import deque, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from causal_gym import HighwayPCH\n",
    "from imitation.imitate import parse_graph, find_sequential_pi_backdoor, collect_expert_trajectories\n",
    "from imitation.gym_gail.core_net import DiscreteActor, Critic, Discriminator\n",
    "from imitation.gym_gail.causal_gail import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0a8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 25\n",
    "seed = 1\n",
    "train_eps = 1000\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2344c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HighwayPCH(num_steps=num_steps, seed=seed, render_mode='rgb_array')\n",
    "num_actions = env.env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ecf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = parse_graph(env.get_graph)\n",
    "X = {f'X{t}' for t in range(num_steps)}\n",
    "Y = f'Y{num_steps}'\n",
    "obs_prefix = env.env.observed_unobserved_vars[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6de747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_dim = 370  | encode(dummy_obs, 0).size = 370\n"
     ]
    }
   ],
   "source": [
    "Z_sets = find_sequential_pi_backdoor(G, X, Y, obs_prefix)\n",
    "categorical_dims = calc_categorical_dims(env)\n",
    "dummy_obs, _ = env.reset(seed=seed)\n",
    "\n",
    "encode, z_dim, union_tokens, var_dims = build_z_encoder(Z_sets, dummy_obs, categorical_dims)\n",
    "dummy_z = encode(dummy_obs, 0)\n",
    "print('z_dim =', z_dim, ' | encode(dummy_obs, 0).size =', int(np.asarray(dummy_z).size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68234c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode 1/1000...\n",
      "  Episode 1 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 2/1000...\n",
      "  Episode 2 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 3/1000...\n",
      "  Episode 3 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 4/1000...\n",
      "  Episode 4 ended at step 10 (terminated: True, truncated: False).\n",
      "Starting episode 5/1000...\n",
      "  Episode 5 ended at step 25 (terminated: False, truncated: True).\n",
      "Starting episode 6/1000...\n",
      "  Episode 6 ended at step 17 (terminated: True, truncated: False).\n",
      "Starting episode 7/1000...\n"
     ]
    }
   ],
   "source": [
    "records = collect_expert_trajectories(\n",
    "    env,\n",
    "    num_episodes=train_eps,\n",
    "    max_steps=num_steps,\n",
    "    behavioral_policy=None,\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = DiscreteActor(z_dim, num_actions, hidden_size=128).to(device)\n",
    "critic = Critic(z_dim, hidden_size=128).to(device)\n",
    "discriminator = Discriminator(z_dim + num_actions, hidden_size=128, dropout=0.2).to(device)\n",
    "\n",
    "actor_optim = Adam(actor.parameters(), lr=3e-4)\n",
    "critic_optim = Adam(critic.parameters(), lr=1e-4)\n",
    "discriminator_optim = Adam(discriminator.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c7a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5142, 445]) torch.Size([5142]) torch.Size([5142, 450])\n"
     ]
    }
   ],
   "source": [
    "Z_e, A_e, X_e = make_expert_batch(records, encode, num_actions)\n",
    "print(Z_e.shape, A_e.shape, X_e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9790e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avg_env_return': 226.13310888900185,\n",
       " 'avg_D_reward': 0.6900997161865234,\n",
       " 'ppo_actor_loss': -0.010333863086998463,\n",
       " 'ppo_critic_loss': 2.1499789357185364,\n",
       " 'ppo_entropy': 1.6092643439769745,\n",
       " 'ppo_approx_kl': -0.00042894088647489614,\n",
       " 'ppo_clip_frac': 0.0,\n",
       " 'D_loss': 10.738677501678467,\n",
       " 'D_real_mean': -0.0033468197216279805,\n",
       " 'D_fake_mean': -0.006157793221063912,\n",
       " 'D_gp': 0.9353475421667099,\n",
       " 'D_accuracy': 0.5634014457464218,\n",
       " 'ep_lens': [10,\n",
       "  4,\n",
       "  4,\n",
       "  11,\n",
       "  8,\n",
       "  22,\n",
       "  16,\n",
       "  6,\n",
       "  20,\n",
       "  29,\n",
       "  30,\n",
       "  13,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  6,\n",
       "  17,\n",
       "  14,\n",
       "  12,\n",
       "  5,\n",
       "  6,\n",
       "  12,\n",
       "  3,\n",
       "  30,\n",
       "  22,\n",
       "  29,\n",
       "  30,\n",
       "  7,\n",
       "  23,\n",
       "  17],\n",
       " 'n_steps': 416,\n",
       " 'n_episodes': 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = one_training_round(\n",
    "    env,\n",
    "    actor,\n",
    "    critic,\n",
    "    discriminator,\n",
    "    actor_optim,\n",
    "    critic_optim,\n",
    "    discriminator_optim,\n",
    "    encode,\n",
    "    num_actions,\n",
    "    X_e,\n",
    "    expert_records=None,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ppo_clip=0.2,\n",
    "    epochs=4,\n",
    "    minibatch_size=256,\n",
    "    entropy_coeff=2e-2,\n",
    "    value_coeff=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    normalize_adv=True,\n",
    "    loss_type='bce',\n",
    "    gp_lambda=10.0,\n",
    "    d_updates=2,\n",
    "    d_minibatch_size=256,\n",
    "    use_gp=False,\n",
    "    instance_noise_std=0.05,\n",
    "    label_smoothing=0.0,\n",
    "    max_steps=num_steps,\n",
    "    num_episodes=train_eps,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[005] R_env(m)=218.446  R_D(m)=0.689  π: L_actor=-0.016  V: L_critic=1.963  D: L=9.965  acc=0.604\n",
      "[010] R_env(m)=306.771  R_D(m)=0.691  π: L_actor=-0.021  V: L_critic=0.689  D: L=5.970  acc=0.614\n",
      "[015] R_env(m)=374.548  R_D(m)=0.693  π: L_actor=-0.006  V: L_critic=0.417  D: L=1.481  acc=0.614\n",
      "[020] R_env(m)=406.108  R_D(m)=0.669  π: L_actor=-0.008  V: L_critic=0.450  D: L=1.364  acc=0.814\n",
      "[025] R_env(m)=478.097  R_D(m)=0.627  π: L_actor=-0.009  V: L_critic=0.260  D: L=1.306  acc=0.925\n",
      "[030] R_env(m)=500.146  R_D(m)=0.577  π: L_actor=-0.018  V: L_critic=0.203  D: L=1.231  acc=0.952\n",
      "[035] R_env(m)=503.076  R_D(m)=0.511  π: L_actor=-0.001  V: L_critic=0.293  D: L=1.137  acc=0.970\n",
      "[040] R_env(m)=508.143  R_D(m)=0.452  π: L_actor=-0.005  V: L_critic=0.111  D: L=1.013  acc=0.981\n",
      "[045] R_env(m)=508.491  R_D(m)=0.407  π: L_actor=-0.010  V: L_critic=0.184  D: L=0.967  acc=0.980\n",
      "[050] R_env(m)=488.536  R_D(m)=0.365  π: L_actor=-0.023  V: L_critic=0.191  D: L=0.949  acc=0.980\n",
      "[055] R_env(m)=462.367  R_D(m)=0.337  π: L_actor=-0.033  V: L_critic=0.226  D: L=0.925  acc=0.981\n",
      "[060] R_env(m)=460.435  R_D(m)=0.322  π: L_actor=-0.015  V: L_critic=0.179  D: L=0.913  acc=0.969\n",
      "[065] R_env(m)=445.578  R_D(m)=0.335  π: L_actor=0.039  V: L_critic=0.513  D: L=1.028  acc=0.894\n",
      "[070] R_env(m)=441.867  R_D(m)=0.362  π: L_actor=-0.067  V: L_critic=0.727  D: L=1.081  acc=0.883\n",
      "[075] R_env(m)=408.970  R_D(m)=0.430  π: L_actor=-0.006  V: L_critic=0.997  D: L=1.258  acc=0.721\n",
      "[080] R_env(m)=355.117  R_D(m)=0.501  π: L_actor=0.036  V: L_critic=0.748  D: L=1.177  acc=0.804\n",
      "[085] R_env(m)=313.055  R_D(m)=0.547  π: L_actor=-0.004  V: L_critic=0.796  D: L=1.201  acc=0.755\n",
      "[090] R_env(m)=278.844  R_D(m)=0.585  π: L_actor=-0.004  V: L_critic=1.233  D: L=1.290  acc=0.659\n",
      "[095] R_env(m)=284.268  R_D(m)=0.568  π: L_actor=-0.003  V: L_critic=0.773  D: L=1.185  acc=0.772\n",
      "[100] R_env(m)=287.014  R_D(m)=0.556  π: L_actor=-0.004  V: L_critic=0.580  D: L=1.170  acc=0.802\n",
      "[105] R_env(m)=283.818  R_D(m)=0.545  π: L_actor=-0.004  V: L_critic=0.691  D: L=1.171  acc=0.799\n",
      "[110] R_env(m)=298.248  R_D(m)=0.530  π: L_actor=-0.003  V: L_critic=0.990  D: L=1.229  acc=0.748\n",
      "[115] R_env(m)=288.257  R_D(m)=0.537  π: L_actor=-0.006  V: L_critic=0.810  D: L=1.206  acc=0.768\n",
      "[120] R_env(m)=281.165  R_D(m)=0.541  π: L_actor=-0.002  V: L_critic=0.780  D: L=1.216  acc=0.756\n"
     ]
    }
   ],
   "source": [
    "epochs = 120\n",
    "log_every = 5\n",
    "ret_ma = deque(maxlen=20)\n",
    "dret_ma = deque(maxlen=20)\n",
    "\n",
    "for it in range(1, epochs + 1):\n",
    "    stats = one_training_round(\n",
    "        env,\n",
    "        actor,\n",
    "        critic,\n",
    "        discriminator,\n",
    "        actor_optim,\n",
    "        critic_optim,\n",
    "        discriminator_optim,\n",
    "        encode,\n",
    "        num_actions,\n",
    "        X_e,\n",
    "        expert_records=None,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        ppo_clip=0.2,\n",
    "        epochs=4,\n",
    "        minibatch_size=256,\n",
    "        entropy_coeff=2e-2,\n",
    "        value_coeff=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        normalize_adv=True,\n",
    "        loss_type='bce',\n",
    "        gp_lambda=10.0,\n",
    "        d_updates=2,\n",
    "        d_minibatch_size=256,\n",
    "        use_gp=False,\n",
    "        instance_noise_std=0.05,\n",
    "        label_smoothing=0.0,\n",
    "        max_steps=num_steps,\n",
    "        num_episodes=train_eps,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    ret_ma.append(stats['avg_env_return'])\n",
    "    dret_ma.append(stats['avg_D_reward'])\n",
    "\n",
    "    if it % log_every == 0:\n",
    "        print(\n",
    "            f\"[{it:03d}] \"\n",
    "            f\"R_env(m)={np.mean(ret_ma):.3f}  \"\n",
    "            f\"R_D(m)={np.mean(dret_ma):.3f}  \"\n",
    "            f\"pi: L_actor={stats['ppo_actor_loss']:.3f}  \"\n",
    "            f\"V: L_critic={stats['ppo_critic_loss']:.3f}  \"\n",
    "            f\"D: L={stats['D_loss']:.3f}  acc={stats['D_accuracy']:.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb26b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_policy(env, actor, encode, num_episodes=1000, max_steps=num_steps, seed=None):\n",
    "    actor.eval()\n",
    "    returns = []\n",
    "\n",
    "    for e in range(num_episodes):\n",
    "        rs = None if seed is None else seed + e + 1000\n",
    "        obs, _ = env.reset(seed=rs)\n",
    "        t, done, ret = 0, False, 0.0\n",
    "\n",
    "        while not done and t < max_steps:\n",
    "            z = torch.from_numpy(encode(obs, t)).float().unsqueeze(0).to(next(actor.parameters()).device)\n",
    "            a, _, _ = actor.act(z, deterministic=True)\n",
    "            obs, r, terminated, truncated, _ = env.do(lambda _: int(a.item()), show_reward=True)\n",
    "\n",
    "            ret += r\n",
    "            t += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        returns.append(ret)\n",
    "\n",
    "    return float(np.mean(returns)), returns\n",
    "\n",
    "_, imitator_rewards = eval_policy(env, actor, encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expert rewards: mean=676.280, std=205.085\n",
      "Imitator rewards: mean=270.594, std=264.848\n"
     ]
    }
   ],
   "source": [
    "expert_rewards = []\n",
    "reward = 0.0\n",
    "last_ep = records[0]['episode']\n",
    "\n",
    "for r in records:\n",
    "    if r['episode'] != last_ep:\n",
    "        expert_rewards.append(reward)\n",
    "        reward = 0.0\n",
    "        last_ep = r['episode']\n",
    "\n",
    "    reward += r['reward']\n",
    "\n",
    "print(f'Expert rewards: mean={np.mean(expert_rewards):.3f}, std={np.std(expert_rewards):.3f}')\n",
    "print(f'Imitator rewards: mean={np.mean(imitator_rewards):.3f}, std={np.std(imitator_rewards):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
